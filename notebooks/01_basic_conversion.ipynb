{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing X to Zarr Conversion\n",
    "\n",
    "Starting with `MIKEConverter` from `src/zarrcatalogue/converters/mike.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from pathlib import Path\n",
    "from zarrcatalogue.converters.mike import MIKEConverter\n",
    "import mikeio\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Initialize converter\n",
    "converter = MIKEConverter()\n",
    "\n",
    "data_path = Path('../tests/testdata/oresundHD_run1.dfsu')\n",
    "\n",
    "zarr_path = Path('oresundHD_run1.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File not found /teamspace/studios/this_studio/data/oresundHD_run1.dfsu",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convert file with custom chunks and compression\u001b[39;00m\n\u001b[32m      2\u001b[39m chunks = {\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33melements\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m320\u001b[39m}  \u001b[38;5;66;03m# Example chunking\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m metadata = \u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_zarr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzarr_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Print metadata\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConversion metadata:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/notebooks/../src/zarrcatalogue/converters/mike.py:78\u001b[39m, in \u001b[36mMIKEConverter.to_zarr\u001b[39m\u001b[34m(self, input_file, zarr_path, chunks, compression_level, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert MIKE dfsu file to zarr format.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \u001b[33;03m    Dictionary containing metadata about the conversion\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Read MIKE file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m ds = \u001b[43mmikeio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Default chunking if not specified\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/.venv/lib/python3.13/site-packages/mikeio/__init__.py:122\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(filename, items, time, keepdims, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdfs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ext:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmikeio.read() is only supported for dfs files. Use mikeio.open for mesh files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m dfs = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dfs, Mesh):\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmikeio.read() is not supported for Mesh files\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/.venv/lib/python3.13/site-packages/mikeio/__init__.py:182\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(filename, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    177\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a supported format for mikeio.open. Valid formats are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_formats\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m     )\n\u001b[32m    180\u001b[39m reader_klass = READERS[ext]\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader_klass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/.venv/lib/python3.13/site-packages/mikeio/dfsu/_factory.py:43\u001b[39m, in \u001b[36mDfsu.__new__\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m | Path) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdfsu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/.venv/lib/python3.13/site-packages/mikeio/dfsu/_factory.py:27\u001b[39m, in \u001b[36mdfsu\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdfsu\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m | Path) -> Any:\n\u001b[32m     26\u001b[39m     filename = \u001b[38;5;28mstr\u001b[39m(filename)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     dfs = \u001b[43mDfsuFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mtype\u001b[39m = DfsuFileType(dfs.DfsuFileType)\n\u001b[32m     29\u001b[39m     dfs.Close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/.venv/lib/python3.13/site-packages/mikecore/DfsuFile.py:294\u001b[39m, in \u001b[36mDfsuFile.Open\u001b[39m\u001b[34m(fileName)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mOpen\u001b[39m(fileName):\n\u001b[32m    293\u001b[39m   dfs = DfsFile();\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m   \u001b[43mdfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDfsFileMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRead\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[32m    295\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (DfsuFile(dfs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/DataCatalogue/.venv/lib/python3.13/site-packages/mikecore/DfsFile.py:710\u001b[39m, in \u001b[36mDfsFile.Open\u001b[39m\u001b[34m(self, filename, mode, parameters)\u001b[39m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m.Close()\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(filename)):\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile not found \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(filename))\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# Check if trying to edit a read-only file.\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((mode == DfsFileMode.Edit \u001b[38;5;129;01mor\u001b[39;00m mode == DfsFileMode.Append) \n\u001b[32m    714\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os.access(filename, os.W_OK))):\n",
      "\u001b[31mException\u001b[39m: File not found /teamspace/studios/this_studio/data/oresundHD_run1.dfsu"
     ]
    }
   ],
   "source": [
    "# Convert file with custom chunks and compression\n",
    "chunks = {'time': 2, 'elements': 320}  # Example chunking\n",
    "metadata = converter.to_zarr(\n",
    "    data_path, \n",
    "    zarr_path,\n",
    "    chunks=chunks,\n",
    "    compression_level=7\n",
    ")\n",
    "\n",
    "# Print metadata\n",
    "print(\"\\nConversion metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Validate conversion\n",
    "validation_results = converter.validate_conversion(data_path, zarr_path)\n",
    "print(\"\\nValidation results:\")\n",
    "for key, value in validation_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Examine Zarr structure\n",
    "store = zarr.open(zarr_path, 'r')\n",
    "print(\"\\nZarr structure:\")\n",
    "print(store.tree())\n",
    "\n",
    "# Basic data validation\n",
    "print(\"\\nData validation:\")\n",
    "original_ds = mikeio.read(data_path)\n",
    "zarr_store = zarr.open(zarr_path, 'r')\n",
    "\n",
    "# Compare first timestep of first variable\n",
    "var_name = original_ds.names[0]\n",
    "original_data = original_ds[var_name].to_numpy()[0]\n",
    "zarr_data = zarr_store[f'data/{var_name}'][0]\n",
    "\n",
    "print(f\"\\nComparing {var_name} data:\")\n",
    "print(f\"Original shape: {original_data.shape}\")\n",
    "print(f\"Zarr shape: {zarr_data.shape}\")\n",
    "print(f\"Max difference: {np.max(np.abs(original_data - zarr_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding to catalogue \n",
    "\n",
    "## single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# Example usage with proper JSON serialization\n",
    "from pathlib import Path\n",
    "from zarrcatalogue.catalog import SimulationCatalog\n",
    "import json\n",
    "\n",
    "# First, let's clean up any corrupted files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "catalog_path = Path(\"/teamspace/studios/this_studio/catalog\")\n",
    "if catalog_path.exists():\n",
    "    shutil.rmtree(catalog_path)\n",
    "\n",
    "# Initialize catalog\n",
    "catalog = SimulationCatalog(catalog_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a simulation\n",
    "simulation_entry = catalog.add_simulation(\n",
    "    sim_id=\"basin_2dv_20241210\",\n",
    "    source_file=Path('/teamspace/studios/this_studio/data/basin_2dv.dfsu'),\n",
    "    metadata={\n",
    "        \"scenario\": \"baseline\",\n",
    "        \"model_version\": \"2.2.0\",\n",
    "        \"description\": \"Vertical profile simulation\"\n",
    "    },\n",
    "    tags=[\"vertical_profile\", \"baseline\"]\n",
    ")\n",
    "\n",
    "# Print the entry using the custom encoder\n",
    "print(\"Added simulation:\")\n",
    "print(json.dumps(simulation_entry, indent=2, cls=CustomJSONEncoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# Example usage with proper JSON serialization\n",
    "from pathlib import Path\n",
    "from zarrcatalogue.catalog import SimulationCatalog\n",
    "import json\n",
    "\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize catalog\n",
    "catalog = SimulationCatalog(Path(\"/teamspace/studios/this_studio/catalog\"))\n",
    "\n",
    "# Optional: Define a metadata generator function\n",
    "def generate_metadata(file_path: Path) -> Dict:\n",
    "    \"\"\"Generate metadata from file path.\"\"\"\n",
    "    return {\n",
    "        \"source_file\": str(file_path),\n",
    "        \"scenario\": file_path.stem.split('_')[0],\n",
    "        \"date_processed\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Bulk import simulations\n",
    "results = catalog.bulk_import(\n",
    "    source_dir=Path(\"/teamspace/studios/this_studio/data\"),\n",
    "    pattern=\"*.dfsu\",\n",
    "    metadata_generator=generate_metadata,\n",
    "    tags=[\"bulk_import\", \"2024\"],\n",
    "    parallel=True,\n",
    "    max_workers=4,\n",
    "    skip_existing=True\n",
    ")\n",
    "\n",
    "# Print successful imports\n",
    "print(\"\\nSuccessfully imported simulations:\")\n",
    "for entry in results[\"successful\"]:\n",
    "    print(f\"- {entry['id']}: {entry['source_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search, Analyze catalogue\n",
    "\n",
    "## summary / overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zarrcatalogue.catalog import SimulationCatalog\n",
    "\n",
    "catalog = SimulationCatalog(Path(\"/teamspace/studios/this_studio/catalog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalog.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search catalog\n",
    "results = catalog.search(\n",
    "    geometry_type=\"GeometryFM2D\",\n",
    "    #variables=[\"U velocity\"],\n",
    "    #tags=[\"baseline\"]\n",
    ")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zarr to MIKE Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from pathlib import Path\n",
    "from zarrcatalogue.converters.mike import MIKEConverter\n",
    "\n",
    "\n",
    "converter = MIKEConverter()\n",
    "metadata = converter.from_zarr(\n",
    "    #zarr_path=Path(\"/teamspace/studios/this_studio/catalog/simulations/basin_2dv/data.zarr\"),\n",
    "    #output_file=Path(\"/teamspace/studios/this_studio/data/basin_2dv_backconversion.dfsu\")\n",
    "    zarr_path=Path(\"/teamspace/studios/this_studio/catalog/simulations/oresundHD_run1/data.zarr\"),\n",
    "    output_file=Path(\"/teamspace/studios/this_studio/data/oresundHD_run1_backconversion.dfsu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the converted file and compare to original\n",
    "import mikeio\n",
    "\n",
    "ds = mikeio.read(\"/teamspace/studios/this_studio/data/oresundHD_run1.dfsu\")\n",
    "ds_backconversion = mikeio.read(\"/teamspace/studios/this_studio/data/oresundHD_run1_backconversion.dfsu\")\n",
    "display(ds, ds_backconversion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
